# ===========================
# Tiny-ImageNet 64x64 Wide ResNet-34 + POT-QAT (Stable)
# ===========================

import os, random, json
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

# -----------------------------
# Flags
# -----------------------------
TRAIN_FP = False
TRAIN_POT = False
TEST = True

# -----------------------------
# Config
# -----------------------------
SEED = 42
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 200
BATCH_SIZE = 64
NUM_WORKERS = 4 if DEVICE.type=="cuda" else 0
PIN_MEMORY = DEVICE.type=="cuda"
EPOCHS_FP = 100
EPOCHS_POT = 30
LR_FP = 0.01
LR_POT = 1e-4
WEIGHT_DECAY = 1e-4
MIXUP_ALPHA = 0.2

# Quantization: powers of 2
POT_MIN_EXP = -8
POT_MAX_EXP = 8
POT_VALUES = [0.0] + [2.0**k for k in range(POT_MIN_EXP, POT_MAX_EXP+1)] + [-(2.0**k) for k in range(POT_MIN_EXP, POT_MAX_EXP+1)]
POT_VALUES = sorted(set(POT_VALUES))

# Saving / export
RUN_DIR = Path("./tinyimagenet_wrn_pot_run")
RUN_DIR.mkdir(parents=True, exist_ok=True)
CKPT_BEST_FP = RUN_DIR / "best_model_fp.pth"
CKPT_FINAL_POT = RUN_DIR / "final_model_pot.pth"
EXPORT_NPZ = RUN_DIR / "weights_pot.npz"
EXPORT_TXT = RUN_DIR / "weights_pot.txt"
META_JSON = RUN_DIR / "model_meta.json"

# -----------------------------
# Repro
# -----------------------------
def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
set_seed()

# -----------------------------
# Dataset
# -----------------------------
print("Loading Tiny-ImageNet 64x64…")
ds = load_dataset("zh-plus/tiny-imagenet")

transform_train = transforms.Compose([
    transforms.RandomResizedCrop(64, scale=(0.7,1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.1,0.1,0.1,0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

transform_val = transforms.Compose([
    transforms.Resize((64,64)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),
])

def collate_fn(batch, train=True):
    images = []
    for item in batch:
        img = item["image"].convert("RGB")   # ensure 3 channels
        img = transform_train(img) if train else transform_val(img)
        images.append(img)
    images = torch.stack(images, dim=0)
    labels = torch.tensor([item["label"] for item in batch], dtype=torch.long)
    return images, labels

train_loader = DataLoader(ds["train"], batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=lambda b: collate_fn(b, train=True),
                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

val_loader = DataLoader(ds["valid"], batch_size=BATCH_SIZE, shuffle=False,
                        collate_fn=lambda b: collate_fn(b, train=False),
                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

# -----------------------------
# Wide ResNet-34
# -----------------------------
class FPConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.ReLU(inplace=True)
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

class ResidualBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv1 = FPConvBlock(in_ch, out_ch, stride)
        self.conv2 = FPConvBlock(out_ch, out_ch)
        self.skip = nn.Identity() if in_ch==out_ch and stride==1 else nn.Conv2d(in_ch,out_ch,1,stride=stride,bias=False)
    def forward(self, x):
        return self.conv2(self.conv1(x)) + self.skip(x)

class WideResNet34(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super().__init__()
        self.stem = FPConvBlock(3,64)
        self.layer1 = nn.Sequential(*[ResidualBlock(64,64) for _ in range(3)])
        self.layer2 = nn.Sequential(*[ResidualBlock(64,128,stride=2)]+[ResidualBlock(128,128) for _ in range(2)])
        self.layer3 = nn.Sequential(*[ResidualBlock(128,256,stride=2)]+[ResidualBlock(256,256) for _ in range(5)])
        self.layer4 = nn.Sequential(*[ResidualBlock(256,512,stride=2)]+[ResidualBlock(512,512) for _ in range(2)])
        self.gap = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(512,num_classes)
    def forward(self,x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.gap(x)
        x = torch.flatten(x,1)
        return self.fc(x)

# -----------------------------
# POT utilities
# -----------------------------
def quantize_pot_tensor(tensor):
    with torch.no_grad():
        pot_tensor = torch.tensor(POT_VALUES, device=tensor.device, dtype=tensor.dtype)
        flat = tensor.view(-1,1)
        diff = (flat - pot_tensor.view(1,-1)).abs()
        idx = diff.argmin(dim=-1)
        tensor.copy_(pot_tensor[idx].view(tensor.shape))

class POTWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    def forward(self, x):
        orig_weights = {name: param.data.clone() for name,param in self.model.named_parameters() if "weight" in name}
        for name,param in self.model.named_parameters():
            if "weight" in name:
                quantize_pot_tensor(param)
        out = self.model(x)
        for name,param in self.model.named_parameters():
            if "weight" in name:
                param.data.copy_(orig_weights[name])
        return out

# -----------------------------
# Training helpers
# -----------------------------
def evaluate(model, loader):
    model.eval()
    correct,total=0,0
    with torch.no_grad():
        for imgs,labels in loader:
            imgs,labels = imgs.to(DEVICE), labels.to(DEVICE)
            pred = model(imgs).argmax(1)
            correct += (pred==labels).sum().item()
            total += labels.size(0)
    return 100.0*correct/max(1,total)

def mixup_data(x, y, alpha=MIXUP_ALPHA):
    if alpha <= 0: return x, y, 1.0
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam*x + (1-lam)*x[index,:]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam*criterion(pred,y_a)+(1-lam)*criterion(pred,y_b)

# -----------------------------
# Pipeline
# -----------------------------
model = WideResNet34().to(DEVICE)

# -----------------------------
# Phase 1: FP training
# -----------------------------
if TRAIN_FP:
    print("\n=== Phase 1: Full-Precision Training ===")
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.SGD(model.parameters(), lr=LR_FP, momentum=0.9, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FP)
    best_acc=0.0
    for epoch in range(1,EPOCHS_FP+1):
        model.train(); running=0.0
        pbar = tqdm(train_loader, desc=f"FP Epoch {epoch}/{EPOCHS_FP}")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            imgs, y_a, y_b, lam = mixup_data(imgs, labels)
            optimizer.zero_grad(set_to_none=True)
            logits = model(imgs)
            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()
            running += loss.item()
            pbar.set_postfix(loss=f"{running/(pbar.n or 1):.4f}")
        scheduler.step()
        val_acc = evaluate(model,val_loader)
        print(f"[FP] Epoch {epoch} val_acc={val_acc:.2f}%")
        if val_acc>best_acc:
            best_acc=val_acc
            torch.save(model.state_dict(), CKPT_BEST_FP)
            print(f" ↳ New best {best_acc:.2f}% — saved FP checkpoint")
    print(f"FP training done. Best val_acc={best_acc:.2f}%")

# -----------------------------
# Phase 2: POT fine-tuning
# -----------------------------
if TRAIN_POT:
    print("\n=== Phase 2: Hard POT Quantization (Shift-only) ===")
    model.load_state_dict(torch.load(CKPT_BEST_FP,map_location=DEVICE))
    pot_model = POTWrapper(model).to(DEVICE)
    optimizer_pot = optim.Adam(model.parameters(), lr=LR_POT, weight_decay=WEIGHT_DECAY)
    best_acc_pot=0.0
    for epoch in range(1,EPOCHS_POT+1):
        model.train(); running=0.0
        pbar = tqdm(train_loader, desc=f"POT Epoch {epoch}/{EPOCHS_POT}")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            optimizer_pot.zero_grad(set_to_none=True)
            logits = pot_model(imgs)
            loss = nn.CrossEntropyLoss()(logits, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer_pot.step()
            running += loss.item()
            pbar.set_postfix(loss=f"{running/(pbar.n or 1):.4f}")
        # hard-quantize weights at end of epoch
        for name,param in model.named_parameters():
            if "weight" in name:
                quantize_pot_tensor(param)
        val_acc = evaluate(model,val_loader)
        print(f"[POT] Epoch {epoch} val_acc={val_acc:.2f}%")
        if val_acc>best_acc_pot:
            best_acc_pot=val_acc
            torch.save(model.state_dict(), CKPT_FINAL_POT)
            print(f" ↳ New best {best_acc_pot:.2f}% — saved POT checkpoint")
    print(f"POT fine-tuning done. Best val_acc={best_acc_pot:.2f}%")

# -----------------------------
# Testing
# -----------------------------
if TEST:
    print("\n=== Testing final POT model ===")
    model.load_state_dict(torch.load(CKPT_FINAL_POT,map_location=DEVICE))
    model.eval()
    pot_model = POTWrapper(model).to(DEVICE)
    
    # Random batch of validation images
    indices = torch.randperm(len(ds["valid"]))[:BATCH_SIZE].tolist()
    batch = [ds["valid"][i] for i in indices]
    imgs, labels = collate_fn(batch, train=False)
    imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
    
    with torch.no_grad():
        preds = pot_model(imgs).argmax(1)
    val_acc = (preds==labels).float().mean().item()*100.0
    print(f"[POT] Random batch val_acc={val_acc:.2f}%")
    print("Sample predictions:", preds[:10])
    print("Ground truth labels:", labels[:10])


