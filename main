# ===========================
# Tiny-ImageNet 64x64 Wide ResNet-34 + POT-QAT (Teacher-Student KD, export)
# ===========================



import os
import random
import json
from pathlib import Path
from copy import deepcopy
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

# -----------------------------
# Flags
# -----------------------------
TRAIN_FP = False          # Phase 1: Train full-precision teacher
TRAIN_POT = True         # Phase 2: POT QAT (student) with KD
TEST = True            # Run a small test at the end
SAVE_FP = False           # Save FP teacher checkpoint
SAVE_POT = True           # Save final POT student checkpoint
EXPORT_POT = True          # Export weights to .npz/.txt

# -----------------------------
# Config
# -----------------------------
SEED = 42
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 200
BATCH_SIZE = 64
NUM_WORKERS = 4 if DEVICE.type == "cuda" else 0
PIN_MEMORY = DEVICE.type == "cuda"
EPOCHS_FP = 150
EPOCHS_POT = 60
LR_FP = 0.01
LR_POT = 3e-4
WEIGHT_DECAY = 1e-4
MIXUP_ALPHA = 0.2

# Quantization: powers of 2
POT_MIN_EXP = -8
POT_MAX_EXP = 8
POT_VALUES = [0.0] + [2.0 ** k for k in range(POT_MIN_EXP, POT_MAX_EXP + 1)] + [-(2.0 ** k) for k in range(POT_MIN_EXP, POT_MAX_EXP + 1)]
POT_VALUES = sorted(set(POT_VALUES))

# Saving / export
RUN_DIR = Path("./tinyimagenet_wrn_pot_run")
RUN_DIR.mkdir(parents=True, exist_ok=True)
CKPT_BEST_FP = RUN_DIR / "best_model_fp.pth"
CKPT_FINAL_POT = RUN_DIR / "final_model_pot.pth"
EXPORT_NPZ = RUN_DIR / "weights_pot.npz"
EXPORT_TXT = RUN_DIR / "weights_pot.txt"
META_JSON = RUN_DIR / "model_meta.json"

# -----------------------------
# Repro
# -----------------------------
def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed()

# -----------------------------
# Dataset
# -----------------------------
print("Loading Tiny-ImageNet 64x64…")
ds = load_dataset("zh-plus/tiny-imagenet")

transform_train = transforms.Compose([
    transforms.RandomResizedCrop(64, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

transform_val = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])


def collate_fn(batch, train=True):
    images = []
    for item in batch:
        img = item["image"].convert("RGB")
        img = transform_train(img) if train else transform_val(img)
        images.append(img)
    images = torch.stack(images, dim=0)
    labels = torch.tensor([item["label"] for item in batch], dtype=torch.long)
    return images, labels

train_loader = DataLoader(ds["train"], batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=lambda b: collate_fn(b, train=True),
                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

val_loader = DataLoader(ds["valid"], batch_size=BATCH_SIZE, shuffle=False,
                        collate_fn=lambda b: collate_fn(b, train=False),
                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

# -----------------------------
# Wide ResNet-34 (simple WRN-style)
# -----------------------------
class FPConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.ReLU(inplace=True)
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

class ResidualBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv1 = FPConvBlock(in_ch, out_ch, stride)
        self.conv2 = FPConvBlock(out_ch, out_ch)
        self.skip = nn.Identity() if in_ch == out_ch and stride == 1 else nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)
    def forward(self, x):
        return self.conv2(self.conv1(x)) + self.skip(x)

class WideResNet34(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super().__init__()
        self.stem = FPConvBlock(3, 64)
        self.layer1 = nn.Sequential(*[ResidualBlock(64, 64) for _ in range(3)])
        self.layer2 = nn.Sequential(*[ResidualBlock(64, 128, stride=2)] + [ResidualBlock(128, 128) for _ in range(2)])
        self.layer3 = nn.Sequential(*[ResidualBlock(128, 256, stride=2)] + [ResidualBlock(256, 256) for _ in range(5)])
        self.layer4 = nn.Sequential(*[ResidualBlock(256, 512, stride=2)] + [ResidualBlock(512, 512) for _ in range(2)])
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes, bias=False)
    def forward(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.gap(x)
        x = torch.flatten(x, 1)
        return self.fc(x)

# -----------------------------
# Fast POT quantizer + projector (Conv/Linear only)
# -----------------------------

def quantize_pot_inplace_(W, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP, dead_zone=None):
    """In-place quantize tensor to {0, +/-2^k} range. Vectorized."""
    with torch.no_grad():
        absW = W.abs()
        if dead_zone is None:
            dead_zone = 2.0 ** (kmin - 1)
        mask_nz = absW >= dead_zone
        # For log2, avoid zeros by clamping
        clamped = absW.clamp(min=2.0 ** kmin, max=2.0 ** kmax)
        k = torch.round(torch.log2(clamped))
        q = torch.sign(W) * torch.pow(2.0, k)
        W.copy_(torch.where(mask_nz, q, torch.zeros_like(W)))

class POTProjector(nn.Module):
    """Project weights to POT for forward (STE-like). Maintains float masters outside forward.
    Targets only Conv2d and Linear weights. Call hard_apply() to permanently quantize.
    """
    def __init__(self, model, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP, dead_zone=None):
        super().__init__()
        self.model = model
        self.kmin = kmin
        self.kmax = kmax
        self.dead_zone = dead_zone
        # Build target list
        self.targets = []
        for m in self.model.modules():
            if isinstance(m, (nn.Conv2d, nn.Linear)):
                if hasattr(m, 'weight') and m.weight is not None:
                    self.targets.append((m, 'weight'))

    def forward(self, x):
        # Backup float weights
        float_backups = []
        for m, pname in self.targets:
            p = getattr(m, pname)
            float_backups.append(p.data.clone())
            quantize_pot_inplace_(p.data, self.kmin, self.kmax, self.dead_zone)

        out = self.model(x)

        # Restore float weights
        for (m, pname), fp in zip(self.targets, float_backups):
            getattr(m, pname).data.copy_(fp)

        return out

    def hard_apply(self):
        """Permanently quantize target weights (in-place). Use before final export/eval."""
        for m, pname in self.targets:
            p = getattr(m, pname)
            quantize_pot_inplace_(p.data, self.kmin, self.kmax, self.dead_zone)

# -----------------------------
# Training helpers
# -----------------------------

def evaluate(model, loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            pred = model(imgs).argmax(1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
    return 100.0 * correct / max(1, total)


def mixup_data(x, y, alpha=MIXUP_ALPHA):
    if alpha <= 0:
        return x, y, 1.0
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# -----------------------------
# Pipeline
# -----------------------------
model = WideResNet34().to(DEVICE)

# -----------------------------
# Phase 1: FP training (Teacher)
# -----------------------------
if TRAIN_FP:
    print("\n=== Phase 1: Full-Precision Training ===")
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.SGD(model.parameters(), lr=LR_FP, momentum=0.9, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FP)
    best_acc = 0.0
    for epoch in range(1, EPOCHS_FP + 1):
        model.train(); running = 0.0
        pbar = tqdm(train_loader, desc=f"FP Epoch {epoch}/{EPOCHS_FP}")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            imgs, y_a, y_b, lam = mixup_data(imgs, labels)
            optimizer.zero_grad(set_to_none=True)
            logits = model(imgs)
            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()
            running += loss.item()
            pbar.set_postfix(loss=f"{running/(pbar.n or 1):.4f}")
        scheduler.step()
        val_acc = evaluate(model, val_loader)
        print(f"[FP] Epoch {epoch} val_acc={val_acc:.2f}%")
        if val_acc > best_acc:
            best_acc = val_acc
            if SAVE_FP:
                torch.save(model.state_dict(), CKPT_BEST_FP)
                print(f" ↳ New best {best_acc:.2f}% — saved FP checkpoint")
    print(f"FP training done. Best val_acc={best_acc:.2f}%")

# -----------------------------
# Phase 2: POT fine-tuning with KD (Student)
# -----------------------------
if TRAIN_POT:
    print("\n=== Phase 2: POT QAT + Knowledge Distillation ===")
    # Load teacher
    teacher = WideResNet34().to(DEVICE)
    if SAVE_FP and CKPT_BEST_FP.exists():
        teacher.load_state_dict(torch.load(CKPT_BEST_FP, map_location=DEVICE))
    else:
        # if no saved checkpoint, use current model weights (assumes TRAIN_FP just ran)
        teacher.load_state_dict(model.state_dict())
    teacher.eval()

    # Student (same arch) initialized from teacher
    student = WideResNet34().to(DEVICE)
    student.load_state_dict(teacher.state_dict())
    pot_student = POTProjector(student, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP)
    pot_student.to(DEVICE)

    optimizer_pot = optim.AdamW(student.parameters(), lr=LR_POT, weight_decay=WEIGHT_DECAY)
    sched = optim.lr_scheduler.CosineAnnealingLR(optimizer_pot, T_max=EPOCHS_POT)

    # KD hyperparams
    tau = 2.0
    alpha_kd = 0.4
    ce = nn.CrossEntropyLoss()

    def kd_loss_fn(s_logits, t_logits, y, tau, alpha):
        ce_loss = ce(s_logits, y)
        log_p_s = torch.log_softmax(s_logits / tau, dim=1)
        p_t = torch.softmax(t_logits / tau, dim=1)
        kl = torch.nn.functional.kl_div(log_p_s, p_t, reduction='batchmean') * (tau * tau)
        return alpha * ce_loss + (1.0 - alpha) * kl

    best_acc_pot = 0.0
    # Keep a float-master backup of student weights for safe eval/restore
    float_master = deepcopy(student.state_dict())

    for epoch in range(1, EPOCHS_POT + 1):
        student.train()
        running = 0.0
        pbar = tqdm(train_loader, desc=f"POT+KD Epoch {epoch}/{EPOCHS_POT}")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)

            # MixUp (if used) - run teacher on same mixed images for consistent targets
            if MIXUP_ALPHA > 0:
                imgs_m, y_a, y_b, lam = mixup_data(imgs, labels, alpha=MIXUP_ALPHA)
                with torch.no_grad():
                    t_logits = teacher(imgs_m)
                optimizer_pot.zero_grad(set_to_none=True)
                s_logits = pot_student(imgs_m)
                loss = lam * kd_loss_fn(s_logits, t_logits, y_a, tau, alpha_kd) + (1 - lam) * kd_loss_fn(s_logits, t_logits, y_b, tau, alpha_kd)
            else:
                with torch.no_grad():
                    t_logits = teacher(imgs)
                optimizer_pot.zero_grad(set_to_none=True)
                s_logits = pot_student(imgs)
                loss = kd_loss_fn(s_logits, t_logits, labels, tau, alpha_kd)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(student.parameters(), 5.0)
            optimizer_pot.step()

            running += loss.item()
            pbar.set_postfix(loss=f"{running/(pbar.n or 1):.4f}")

        sched.step()

        # Evaluate: make a quantized clone to avoid destroying float masters
        eval_student = deepcopy(student)
        eval_projector = POTProjector(eval_student, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP)
        eval_projector.to(DEVICE)
        eval_projector.hard_apply()
        eval_student.to(DEVICE)
        val_acc = evaluate(eval_student, val_loader)

        print(f"[POT] Epoch {epoch} val_acc={val_acc:.2f}%")

        if val_acc > best_acc_pot:
            best_acc_pot = val_acc
            if SAVE_POT:
                # permanently quantize the working student and save
                pot_student.hard_apply()
                torch.save(student.state_dict(), CKPT_FINAL_POT)
                # restore float weights from float_master (to continue training with float masters)
                student.load_state_dict(float_master)
                print(f" ↳ New best {best_acc_pot:.2f}% — saved POT checkpoint")

        # Optionally hard-quantize every epoch (not recommended for QAT; kept behind flag)
        
    print(f"POT KD fine-tuning done. Best val_acc={best_acc_pot:.2f}%")

    # Finalize: hard-apply quantization and export/save
    if SAVE_POT or EXPORT_POT:
        pot_student.hard_apply()
        if SAVE_POT:
            torch.save(student.state_dict(), CKPT_FINAL_POT)
        if EXPORT_POT:
            # export to npz + human-readable txt
            def export_pot_weights(model, npz_path, txt_path, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP, dead_zone=None):
                out_state = {}
                lines = []
                for name, p in model.named_parameters():
                    W = p.detach().cpu().clone()
                    out_state[name] = W.numpy()

                    if ("weight" in name) and (('conv' in name.lower() or 'fc' in name.lower() or 'linear' in name.lower())):
                        # quantize copy of the weights
                        Wq = W.clone()
                        quantize_pot_inplace_(Wq, kmin, kmax, dead_zone)
                        nonzeros = int((Wq != 0).sum())
                        # add header
                        lines.append(f"{name} shape={tuple(Wq.shape)} nonzeros={nonzeros}")
                        # flatten weights for readability
                        flat_vals = Wq.flatten().tolist()
                        # write as space-separated string (limit line length)
                        chunk_size = 16
                        for i in range(0, len(flat_vals), chunk_size):
                            chunk = flat_vals[i:i+chunk_size]
                            lines.append(" ".join([f"{v:.6g}" for v in chunk]))

                # save npz
                np.savez(npz_path, **out_state)

                # save txt
                with open(txt_path, 'w') as f:
                    f.write('\n'.join(lines))

                # write a small meta
                meta = {
                    'kmin': kmin,
                    'kmax': kmax,
                    'dead_zone': float(2.0 ** (kmin - 1)),
                    'num_classes': NUM_CLASSES
                }
                with open(META_JSON, 'w') as f:
                    json.dump(meta, f, indent=2)

            export_pot_weights(student, EXPORT_NPZ, EXPORT_TXT)
            print(f"Exported POT weights to {EXPORT_NPZ} and {EXPORT_TXT}")

# -----------------------------
# Testing / quick sanity check
# -----------------------------
if TEST:
    print("\n=== Testing final POT model ===")
    if CKPT_FINAL_POT.exists():
        student.load_state_dict(torch.load(CKPT_FINAL_POT, map_location=DEVICE))
    student.eval()
    pot_for_test = POTProjector(student, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP)
    pot_for_test.to(DEVICE)

    # Random batch of validation images
    indices = torch.randperm(len(ds['valid']))[:BATCH_SIZE].tolist()
    batch = [ds['valid'][i] for i in indices]
    imgs, labels = collate_fn(batch, train=False)
    imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)

    with torch.no_grad():
        preds = pot_for_test(imgs).argmax(1)
    val_acc = (preds == labels).float().mean().item() * 100.0
    print(f"[POT] Random batch val_acc={val_acc:.2f}%")
    print("Sample predictions:", preds[:10])
    print("Ground truth labels:", labels[:10])

# -----------------------------
# End
# -----------------------------
print("Pipeline finished.")




