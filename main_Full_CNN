# Tiny-ImageNet 64x64 Wide ResNet-34 + POT-QAT (Teacher-Student KD, export)
# Full pipeline script with flags, saving, export, and safer eval cloning


import os
import random
import json
from pathlib import Path
from copy import deepcopy
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from datasets import load_dataset
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------
# Flags
# -----------------------------
TRAIN_FP = False         # Phase 1: Train full-precision teacher
TRAIN_POT = True        # Phase 2: POT QAT (student) with KD
TEST = False            # Run a small test at the end
SAVE_FP = False          # Save FP teacher checkpoint
SAVE_POT = False         # Save final POT student checkpoint
EXPORT_POT = False       # Export weights to .npz/.txt

# -----------------------------
# Config
# -----------------------------
SEED = 42
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 200
BATCH_SIZE = 64
NUM_WORKERS = 4 if DEVICE.type == "cuda" else 0
PIN_MEMORY = DEVICE.type == "cuda"
EPOCHS_FP = 150
EPOCHS_POT =80
LR_FP = 0.01
LR_POT = 3e-4
WEIGHT_DECAY = 5e-4
MIXUP_ALPHA = 0.2

# Quantization: powers of 2
POT_MIN_EXP = -8
POT_MAX_EXP = 8

# Saving / export
RUN_DIR = Path("./tinyimagenet_wrn_pot_run")
RUN_DIR.mkdir(parents=True, exist_ok=True)
CKPT_BEST_FP = RUN_DIR / "best_model_fp.pth"
CKPT_FINAL_POT = RUN_DIR / "final_model_pot.pth"
EXPORT_NPZ = RUN_DIR / "weights_pot.npz"
EXPORT_TXT = RUN_DIR / "weights_pot.txt"
META_JSON = RUN_DIR / "model_meta.json"

# -----------------------------
# Repro
# -----------------------------
def set_seed(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
set_seed()

# -----------------------------
# Dataset
# -----------------------------
print("Loading Tiny-ImageNet 64x64…")
ds = load_dataset("zh-plus/tiny-imagenet")

transform_train = transforms.Compose([
    transforms.RandomResizedCrop(64, scale=(0.7, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

transform_val = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
])

def collate_fn(batch, train=True):
    images = []
    for item in batch:
        img = item["image"].convert("RGB")
        img = transform_train(img) if train else transform_val(img)
        images.append(img)
    images = torch.stack(images, dim=0)
    labels = torch.tensor([item["label"] for item in batch], dtype=torch.long)
    return images, labels

train_loader = DataLoader(ds["train"], batch_size=BATCH_SIZE, shuffle=True,
                          collate_fn=lambda b: collate_fn(b, train=True),
                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

val_loader = DataLoader(ds["valid"], batch_size=BATCH_SIZE, shuffle=False,
                        collate_fn=lambda b: collate_fn(b, train=False),
                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)

# -----------------------------
# Wide ResNet-34 (simple WRN-style)
# -----------------------------
class FPConvBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_ch)
        self.act = nn.ReLU(inplace=True)
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

class ResidualBlock(nn.Module):
    def __init__(self, in_ch, out_ch, stride=1):
        super().__init__()
        self.conv1 = FPConvBlock(in_ch, out_ch, stride)
        self.conv2 = FPConvBlock(out_ch, out_ch)
        self.skip = nn.Identity() if in_ch == out_ch and stride == 1 else nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False)
    def forward(self, x):
        return self.conv2(self.conv1(x)) + self.skip(x)

class WideResNet34(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES):
        super().__init__()
        self.stem = FPConvBlock(3, 64)
        self.layer1 = nn.Sequential(*[ResidualBlock(64, 64) for _ in range(3)])
        self.layer2 = nn.Sequential(*[ResidualBlock(64, 128, stride=2)] + [ResidualBlock(128, 128) for _ in range(2)])
        self.layer3 = nn.Sequential(*[ResidualBlock(128, 256, stride=2)] + [ResidualBlock(256, 256) for _ in range(5)])
        self.layer4 = nn.Sequential(*[ResidualBlock(256, 512, stride=2)] + [ResidualBlock(512, 512) for _ in range(2)])
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes, bias=False)
    def forward(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.gap(x)
        x = torch.flatten(x, 1)
        return self.fc(x)

# -----------------------------
# POT quantizer
# -----------------------------
def quantize_pot_inplace_(W, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP, dead_zone=None):
    with torch.no_grad():
        absW = W.abs()
        if dead_zone is None:
            dead_zone = 2.0 ** (kmin - 1)
        mask_nz = absW >= dead_zone
        clamped = absW.clamp(min=2.0 ** kmin, max=2.0 ** kmax)
        k = torch.round(torch.log2(clamped))
        q = torch.sign(W) * torch.pow(2.0, k)
        W.copy_(torch.where(mask_nz, q, torch.zeros_like(W)))

class POTProjector(nn.Module):
    def __init__(self, model, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP, dead_zone=None):
        super().__init__()
        self.model = model
        self.kmin = kmin
        self.kmax = kmax
        self.dead_zone = dead_zone
        self.targets = []
        for m in self.model.modules():
            if isinstance(m, (nn.Conv2d, nn.Linear)) and hasattr(m, 'weight') and m.weight is not None:
                self.targets.append((m, 'weight'))

    def forward(self, x):
        float_backups = []
        for m, pname in self.targets:
            p = getattr(m, pname)
            float_backups.append(p.data.clone())
            quantize_pot_inplace_(p.data, self.kmin, self.kmax, self.dead_zone)
        out = self.model(x)
        for (m, pname), fp in zip(self.targets, float_backups):
            getattr(m, pname).data.copy_(fp)
        return out

    def hard_apply(self):
        for m, pname in self.targets:
            p = getattr(m, pname)
            quantize_pot_inplace_(p.data, self.kmin, self.kmax, self.dead_zone)

# -----------------------------
# Helpers
# -----------------------------
def evaluate(model, loader):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for imgs, labels in loader:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            pred = model(imgs).argmax(1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)
    return 100.0 * correct / max(1, total)

def mixup_data(x, y, alpha=MIXUP_ALPHA):
    if alpha <= 0:
        return x, y, 1.0
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# -----------------------------
# Pipeline
# -----------------------------
model = WideResNet34().to(DEVICE)

# -----------------------------
# Phase 1: FP training
# -----------------------------
train_losses_fp, train_accs_fp, val_accs_fp = [], [], []

if TRAIN_FP:
    print("\n=== Phase 1: Full-Precision Training ===")
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.SGD(model.parameters(), lr=LR_FP, momentum=0.9, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FP)
    best_acc = 0.0

    for epoch in range(1, EPOCHS_FP + 1):
        model.train()
        running_loss = 0.0
        running_correct = 0
        running_total = 0
        pbar = tqdm(train_loader, desc=f"FP Epoch {epoch}/{EPOCHS_FP}")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            imgs, y_a, y_b, lam = mixup_data(imgs, labels)
            optimizer.zero_grad(set_to_none=True)
            logits = model(imgs)
            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()
            running_loss += loss.item() * imgs.size(0)
            pred = logits.argmax(1)
            running_correct += ((lam * pred.eq(y_a) + (1 - lam) * pred.eq(y_b)).sum()).item()
            running_total += labels.size(0)
            pbar.set_postfix(loss=f"{running_loss/running_total:.4f}")

        epoch_loss = running_loss / running_total
        epoch_acc = 100.0 * running_correct / running_total
        train_losses_fp.append(epoch_loss)
        train_accs_fp.append(epoch_acc)
        val_acc = evaluate(model, val_loader)
        val_accs_fp.append(val_acc)
        print(f"[FP] Epoch {epoch} train_acc={epoch_acc:.2f}% val_acc={val_acc:.2f}%")
        if val_acc > best_acc:
            best_acc = val_acc
            if SAVE_FP:
                torch.save(model.state_dict(), CKPT_BEST_FP)
                print(f" ↳ New best {best_acc:.2f}% — saved FP checkpoint")
    print(f"FP training done. Best val_acc={best_acc:.2f}%")

# -----------------------------
# Phase 2: POT QAT + KD
# -----------------------------
train_losses_pot, train_accs_pot, val_accs_pot = [], [], []

if TRAIN_POT:
    print("\n=== Phase 2: POT QAT + Knowledge Distillation ===")
    teacher = WideResNet34().to(DEVICE)
    if SAVE_FP and CKPT_BEST_FP.exists():
        teacher.load_state_dict(torch.load(CKPT_BEST_FP, map_location=DEVICE))
    else:
        teacher.load_state_dict(model.state_dict())
    teacher.eval()
    student = WideResNet34().to(DEVICE)
    student.load_state_dict(teacher.state_dict())
    pot_student = POTProjector(student, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP)
    pot_student.to(DEVICE)
    optimizer_pot = optim.AdamW(student.parameters(), lr=LR_POT, weight_decay=WEIGHT_DECAY)
    sched = optim.lr_scheduler.CosineAnnealingLR(optimizer_pot, T_max=EPOCHS_POT)

    tau = 2.0
    alpha_kd = 0.4
    ce = nn.CrossEntropyLoss()
    def kd_loss_fn(s_logits, t_logits, y, tau, alpha):
        ce_loss = ce(s_logits, y)
        log_p_s = torch.log_softmax(s_logits / tau, dim=1)
        p_t = torch.softmax(t_logits / tau, dim=1)
        kl = torch.nn.functional.kl_div(log_p_s, p_t, reduction='batchmean') * (tau * tau)
        return alpha * ce_loss + (1.0 - alpha) * kl

    best_acc_pot = 0.0
    float_master = deepcopy(student.state_dict())

    for epoch in range(1, EPOCHS_POT + 1):
        student.train()
        running_loss = 0.0
        running_correct = 0
        running_total = 0
        pbar = tqdm(train_loader, desc=f"POT+KD Epoch {epoch}/{EPOCHS_POT}")
        for imgs, labels in pbar:
            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)
            if MIXUP_ALPHA > 0:
                imgs_m, y_a, y_b, lam = mixup_data(imgs, labels)
                with torch.no_grad():
                    t_logits = teacher(imgs_m)
                optimizer_pot.zero_grad(set_to_none=True)
                s_logits = pot_student(imgs_m)
                loss = lam * kd_loss_fn(s_logits, t_logits, y_a, tau, alpha_kd) + (1 - lam) * kd_loss_fn(s_logits, t_logits, y_b, tau, alpha_kd)
                pred = s_logits.argmax(1)
                running_correct += ((lam * pred.eq(y_a) + (1 - lam) * pred.eq(y_b)).sum()).item()
            else:
                with torch.no_grad():
                    t_logits = teacher(imgs)
                optimizer_pot.zero_grad(set_to_none=True)
                s_logits = pot_student(imgs)
                loss = kd_loss_fn(s_logits, t_logits, labels, tau, alpha_kd)
                pred = s_logits.argmax(1)
                running_correct += (pred.eq(labels).sum().item())

            loss.backward()
            torch.nn.utils.clip_grad_norm_(student.parameters(), 5.0)
            optimizer_pot.step()
            running_loss += loss.item() * imgs.size(0)
            running_total += labels.size(0)
            pbar.set_postfix(loss=f"{running_loss/running_total:.4f}")

        epoch_loss = running_loss / running_total
        epoch_acc = 100.0 * running_correct / running_total
        train_losses_pot.append(epoch_loss)
        train_accs_pot.append(epoch_acc)

        eval_student = deepcopy(student)
        eval_projector = POTProjector(eval_student, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP)
        eval_projector.to(DEVICE)
        eval_projector.hard_apply()
        val_acc = evaluate(eval_student, val_loader)
        val_accs_pot.append(val_acc)
        print(f"[POT] Epoch {epoch} train_acc={epoch_acc:.2f}% val_acc={val_acc:.2f}%")

        if val_acc > best_acc_pot:
            best_acc_pot = val_acc
            if SAVE_POT:
                pot_student.hard_apply()
                torch.save(student.state_dict(), CKPT_FINAL_POT)
                student.load_state_dict(float_master)
                print(f" ↳ New best {best_acc_pot:.2f}% — saved POT checkpoint")

    # Finalize: hard apply + export
    if SAVE_POT or EXPORT_POT:
        pot_student.hard_apply()
        if SAVE_POT:
            torch.save(student.state_dict(), CKPT_FINAL_POT)
        if EXPORT_POT:
            def export_pot_weights(model, npz_path, txt_path, kmin=POT_MIN_EXP, kmax=POT_MAX_EXP, dead_zone=None):
                out_state = {}
                lines = []
                for name, p in model.named_parameters():
                    W = p.detach().cpu().clone()
                    out_state[name] = W.numpy()
                    if ("weight" in name) and (('conv' in name.lower() or 'fc' in name.lower() or 'linear' in name.lower())):
                        Wq = W.clone()
                        quantize_pot_inplace_(Wq, kmin, kmax, dead_zone)
                        nonzeros = int((Wq != 0).sum())
                        lines.append(f"\n# Layer: {name} | shape={tuple(Wq.shape)} | nonzeros={nonzeros}")
                        flat_vals = Wq.flatten().tolist()
                        chunk_size = 16
                        for i in range(0, len(flat_vals), chunk_size):
                            chunk = flat_vals[i:i+chunk_size]
                            row = " ".join([f"{v:.6f}" for v in chunk])
                            lines.append(row)
                np.savez(npz_path, **out_state)
                with open(txt_path, 'w') as f:
                    f.write('\n'.join(lines))
                meta = {'kmin': kmin, 'kmax': kmax, 'dead_zone': float(2.0**(kmin-1)), 'num_classes': NUM_CLASSES}
                with open(META_JSON, 'w') as f:
                    json.dump(meta, f, indent=2)
            export_pot_weights(student, EXPORT_NPZ, EXPORT_TXT)
            print(f"Exported POT weights to {EXPORT_NPZ} and {EXPORT_TXT}")

# -----------------------------
# Plotting
# -----------------------------
if SAVE_FP and SAVE_POT:
  plt.figure(figsize=(8,5))
  plt.plot(range(1, EPOCHS_FP+1), train_losses_fp, label='FP Train Loss')
  plt.plot(range(1, EPOCHS_POT+1), train_losses_pot, label='POT+KD Train Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.title('Training Loss Curve')
  plt.legend()
  plt.grid(True)
  plt.show()

  plt.figure(figsize=(8,5))
  plt.plot(range(1, EPOCHS_FP+1), train_accs_fp, label='FP Train Acc')
  plt.plot(range(1, EPOCHS_FP+1), val_accs_fp, label='FP Val Acc')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy (%)')
  plt.title('FP Training Accuracy')
  plt.legend()
  plt.grid(True)
  plt.show()

  plt.figure(figsize=(8,5))
  plt.plot(range(1, EPOCHS_POT+1), train_accs_pot, label='POT+KD Train Acc')
  plt.plot(range(1, EPOCHS_POT+1), val_accs_pot, label='POT+KD Val Acc')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy (%)')
  plt.title('POT+KD Training Accuracy')
  plt.legend()
  plt.grid(True)
  plt.show()



